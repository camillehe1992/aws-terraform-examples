# Centralized Logging - ECS Service (Daemon) for Fluent-Bit

The project is used to deploy a daemonset service for Fluent-Bit in AWS ECS Cluster to enable centralized container logging with Fluent Bit.

The Terraform resources deployed in the project include:

- An ECS service with scheduling strategy as DAEMON
- An ECS task definition for above service using docker image `camillehe1992/fluent-bit:latest`
- An IAM task role attached on task
- An firehose Delivery Stream destinate to S3

> The creation of S3 bucket for logging data is not covered in the project, which means you have to create the S3 bucket and update variable `firehose_bucket_name` default value in `variables.tf` before applying the project.
>
> The creation of NGINX apps are covered in another project. See `ecs-service-nginx` for the details.

The architecture diagram shows as below. The application-level logs generated by NGINX apps running in each cluster is captured by Fluent Bit and streamed via Amazon Kinesis Data Firehose to Amazon S3, where we can query them using Amazon Athena.

![arch-diagram](./e2e-log-analysis-app.png)

## Project Structure

```bash
.
├── .env.sample                 # file for environment variables
├── .terraform.lock.hcl
├── 01_data.tf                  # All file with .tf extensions are Terraform related
├── 01_local.tf
├── 01_variables.tf
├── 01_versions.tf
├── 02_firehose.tf
├── 02_main.tf
├── 02_roles.tf
├── 03_outputs.tf
├── Dockerfile                  # Dockerfile for custom fluent-bit image
├── Makefile                    # Define several common useful Make scripts
├── README.md
├── fluent-bit.conf             # fluent-bit configuration file
├── parsers.conf                # fluent-bit parsers file
├── tf_dev.tfvars               # Terraform variables per environments
├── tf_prod.tfvars
```

## Fluent Bit Configuration & Parsers

`fluent-bit.conf` file defines the routing to the Firehose delivery stream. Don't forget the update `delivery_stream` field as the real Firehose Delivery Stream name that you created.

`parsers.conf` file defines the NGINX log parsing.

## Build & Publish Docker Image

Use below command to publish Docker image to a docker registry, for example DockerHub.

> The `make create-image` script creates a docker image that is compatiable with both amd64/arm64 Arch OS.

```bash
make create-image
```

Finally, the docker image is published to DockerHub `camillehe1992/fluent-bit:latest` successfully.

## Local Deploy

Create a `.env` from `env.sample`, and update environment variables as needed. The `.env` file won't be checked into your source code. After updated, these variables in `.env` will be injected into `Makefile` when you execute `make` commands. You can run `make check_env` to validate these variables.

Another option to specify value of variable is to provide the value in command which has high priority than `.env`. For example, use `make ENVIRONMENT=prod check_env` to overwrite the `ENVIRONMENT` variable to `prod` instead of `dev` defined in `.env`.

Setup local development and AWS credentials following [README](../README.md) before you can deploy AWS resources using below commands.

```bash
# Create a Terraform plan named `tfplan`
make plan

# Apply the plan `tfplan`
make apply
```

verify the Fluent Bit daemonset by peeking into the logs like so

```text
[2023/11/27 02:50:39] [ info] [cmetrics] version=0.3.7
[2023/11/27 02:50:39] [ info] [input:forward:forward.0] listening on unix:///var/run/fluent.sock
time="2023-11-27T02:50:39Z" level=info msg="[firehose 0] plugin parameter delivery_stream = 'ecs-stream'"
time="2023-11-27T02:50:39Z" level=info msg="[firehose 0] plugin parameter region = 'ap-southeast-1'"
...
[2023/11/27 02:50:39] [ info] [sp] stream processor started
```

## Local Destroy

Run below commands to destroy resouces.

```bash
# Create a Terraform destroy plan named `tfplan`
make plan-destroy

# Apply the destroy plan `tfplan`
make apply
```

## Log Collection from ECS Service NGINX

After deploying Fluent Bit as a DeamonSet in ECS Cluster, deploy a NGINX ECS service in cluster using `ecs-service-nginx` project to demonstrate Fluent Bit functionalities.

![ecs-service](./ecs-services.png)

Access the NGINX via NGINX load balancer DNS name, like `http://nginx-prod-xxxxxxxx.aws-region.elb.amazonaws.com/`. You should be redirected to the welcome page of NGINX web server.

Send requests to NGINX web server. Some log files will be uploaded to S3 bucket after a few minutes.

## Create & Query Athena Table

Open Athena -> Query editor -> Tables and views. Click on `CREATE TABLE` from `Create` dropdown list. Paste below SQL expression into the SQL Query editor, and run the query to create a table named `fluentbit_ecs`.

```sql
CREATE EXTERNAL TABLE fluentbit_ecs (
    agent string,
    code string,
    host string,
    method string,
    path string,
    referer string,
    remote string,
    size string,
    user string
)
ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
LOCATION 's3://firehose-streaming-ecs-logs-using-fluent-bit/'
```

After done, query the table. For example, perform a SQL query to figure out who the top 10 users of our NGINX services are in the ECS cluster.

```sql
SELECT host, remote, path, count(remote) AS num_requests
FROM fluentbit_ecs
GROUP BY host, remote, path
ORDER BY num_requests DESC LIMIT 10
```

## References

- [](https://fluentbit.io/)
- [Centralized Container Logging with Fluent Bit](https://aws.amazon.com/blogs/opensource/centralized-container-logging-fluent-bit/)
- [Task role vs task execution role in Amazon ECS](https://towardsthecloud.com/amazon-ecs-task-role-vs-execution-role)
